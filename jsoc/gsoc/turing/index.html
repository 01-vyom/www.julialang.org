<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>Turing Projects – Summer of Code</title>
<meta name="author" content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al." />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Official website for the Julia programming language. Join the Julia community today.">

<meta property="og:title" content="The Julia Language"/>
<meta property="og:image" content="http://www.julialang.org/images/julia-open-graph.png"/>
<meta property="og:description" content="Official website for the Julia programming language"/>

<link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i" rel="stylesheet">
<link rel="stylesheet" href="https://julialang.org/v2/css/bootstrap.min.css" />
<link rel="stylesheet" href="https://julialang.org/v2/css/app.css" />
<link rel="stylesheet" href="https://julialang.org/v2/css/fonts.css" />
<script async defer src="https://buttons.github.io/buttons.js"></script>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
             extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
  });
</script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-28835595-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


  

  
</head>

<body>
  
  

<div class="container py-3 py-lg-0">
  <nav class="navbar navbar-expand-lg navbar-light bg-light" id="main-menu">

    <a class="navbar-brand" href="../../../" id="logo">
      <img src="https://julialang.org/v2/img/logo.svg" height="55" width="85" alt="JuliaLang Logo"/>
    </a>

    <button class="navbar-toggler ml-auto hidden-sm-up float-xs-left" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="https://julialang.org/downloads/">Download</a>
        </li>
        <li class="nav-item flex-md-fill text-md-center">
          <a class="nav-link" href="https://docs.julialang.org">Documentation</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="https://julialang.org/blog/">Blog</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="https://julialang.org/community/">Community</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="https://julialang.org/learning/">Learning</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="https://julialang.org/research/">Research</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="https://julialang.org/jsoc/">JSoC</a>
        </li>
        <li class="nav-item donate flex-md-fill text-md-center">
          <a class="github-button" href="https://github.com/sponsors/julialang" data-icon="octicon-heart" data-size="large" aria-label="Sponsor @julialang on GitHub">Sponsor</a>
        </li>
      </ul>
    </div>

  </nav>
</div>



  <br /><br/>

  <div class = "container">
  

<h1 id="hahahugoshortcode-s0-hbhb">Turing Projects – Summer of Code</h1>

<p><a href="http://turing.ml/">Turing</a> is a universal probabilistic programming language embedded in Julia. Turing allows the user to write models in standard Julia syntax, and provide a wide range of sampling-based inference methods for solving problems across probabilistic machine learning, Bayesian statistics and data science etc. Since Turing is implemented in pure Julia code, its compiler and inference methods are amenable to hacking: new model families and inference methods can be easily added. Below is a list of ideas for potential projects, though you are welcome to propose your own to the Turing team.</p>

<p>Project mentors are <a href="https://github.com/yebai">Hong Ge</a>, <a href="https://github.com/cpfiffer">Cameron Pfiffer</a>, <a href="https://github.com/trappmartin">Martin Trapp</a>, <a href="https://github.com/willtebbutt">Will Tebbutt</a>, <a href="https://github.com/mohamed82008">Mohamed Tarek</a> and <a href="https://github.com/xukai92">Kai Xu</a>.</p>

<h2 id="benchmarking">Benchmarking</h2>

<p>Turing&rsquo;s performance has been sporadically benchmarked against various other probabilistic programming languages (e.g. Turing, Stan, PyMC3, TensorFlow Prob), but a systemic approach to studying where Turing excels and where it falls short would be useful. A GSoC student would implement identical models in many PPLs and build tools to benchmark all PPLs against one another.</p>

<p><strong>Recommended skills:</strong> An interest in Julia and Turing, as well as experience or desire to learn about various other PPLs. Some experience with automated tasks useful, but not necessary at the outset.</p>

<p><strong>Expected output:</strong> A suite of auto-updating bechmarks that track Turing&rsquo;s performance on models implemented in various languages.</p>

<h2 id="nested-sampling-integration">Nested sampling integration</h2>

<p>Turing focuses on modularity in inference methods, and the development team would like to see more inference methods, particularly the popular nested sampling method. A Julia package (<a href="https://github.com/mileslucas/NestedSamplers.jl">NestedSamplers.jl</a>) but it is not hooked up to Turing and does not currently have a stable API. A GSoC student would either integrate that package or construct their own nested sampling method and build it into Turing.</p>

<p><strong>Recommended skills:</strong> Understanding of inference methods and general probability theory. Nested sampler knowledge useful but not required.</p>

<p><strong>Expected output:</strong> A nested sampler that can be used with Turing models.</p>

<h2 id="automated-function-memoization-by-model-annotation">Automated function memoization by model annotation</h2>

<p>Function memoization is a way to reduce costly function evaluation by caching the output when the same inputs are given. Turing&rsquo;s Gibbs sampler often ends up <a href="https://turing.ml/dev/docs/using-turing/performancetips#reuse-computations-in-gibbs-sampling">rerunning expensive functions</a> multiple times, and it would be a significant performance improvement to allow Turing&rsquo;s model compiler to automatically memoize functions where appropriate. A student working on this project would become intimately familiar with Turing&rsquo;s model compiler and build in various automated improvements.</p>

<p><strong>Recommended skills:</strong> General programming skills, hopefully with an understanding of what makes code perform efficiently.</p>

<p><strong>Expected output:</strong> Additions to the Turing compiler that automatically memoize functions where appropriate.</p>

<h2 id="making-distributions-gpu-compatible">Making Distributions GPU compatible</h2>

<p>Julia&rsquo;s GPU tooling is generally quite good, but currently Turing is not able to reliably use GPUs while sampling because <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> is not GPU compatible. A student on this project would work with the Turing developers and the Distributions developers to allow the use of GPU parallelism where possible in Turing.</p>

<p><strong>Recommended skills:</strong> GPU computing. Understanding of various statistical distributions useful but not required.</p>

<p><strong>Expected output:</strong> A set of Distributions.jl objects where <code>logpdf</code> calls can be easily run through a GPU.</p>

<h2 id="gpnet-extensions">GPnet extensions</h2>

<p>One of Turing&rsquo;s sattelite packages, <a href="https://github.com/TuringLang/GPnet.jl">GPnet</a>, is designed to provide a comprehensive suite of Gaussian process tools. See <a href="https://github.com/TuringLang/GPnet.jl/issues/2">this issue</a> for potential tasks &ndash; there&rsquo;s a lot of interesting stuff going on with GPs, and this task in particular may have some creative freedom to it.</p>

<p><strong>Recommended skills:</strong> Gaussian processes. Some Python required, as GPnet uses PyCall.</p>

<p><strong>Expected output:</strong> Improved GP support. The output is variable depending on the student.</p>

<h2 id="model-comparison-tools">Model comparison tools</h2>

<p>Turing and its sattelite packages do not currently provide a comprehensive suite of model comparison tools, a critical tool for the applied statistician. A student who worked on this project would implement various model comparison tools like <a href="https://mc-stan.org/loo/">LOO and WAIC</a>, among others.</p>

<p><strong>Recommended skills:</strong> General statistics. Bayesian inference and model comparison. Some Julia programming.</p>

<p><strong>Expected output:</strong> An easy-to-use set of model comparison tools that allows Turing users to effortlessly compare multiple models on a variety of metrics.</p>

<h2 id="mle-map-tools">MLE/MAP tools</h2>

<p><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum likelihood estimates</a> (MLE) and <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">maximum a posteriori</a> (MAP) estimates can currently only be done by users through a <a href="https://turing.ml/dev/docs/using-turing/advanced#maximum-a-posteriori-estimation">clunky set of workarounds</a>. A streamlined function like <code>mle(model)</code> or <code>map(model)</code> would be very useful for many of Turing&rsquo;s users who want to see what the MLE or MAP estimates look like, and it may be valuable to allow for functionality that allows MCMC sampling to begin from the MLE or MAP estimates. Students working on this project will work with optimization packages such as <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> to make MLE and MAP estimation straightforward for Turing models.</p>

<p><strong>Recommended skills:</strong> Optimization, familiarity with maximum likelihood or MAP.</p>

<p><strong>Expected output:</strong> <code>map</code> and <code>mle</code> (names pending) functions for Turing that yield maximum likelihood and maximum a posteriori estimates of a model, and potentially statistics about the estimate such as the standard errors.</p>

<h2 id="static-distributions">Static distributions</h2>

<p>Small, fixed-size vectors and matrices are fairly common in Turing models. This means that sampling in Turing can probably benefit from using statically sized vectors and matrices from StaticArrays.jl instead of normal, dynamic Julia arrays. Beside the often superior performance of small static vectors and matrices, static arrays are also automatically compatible with the GPU stack in Julia. Currently, the main obstacle to using StaticArrays.jl is that distributions in Distributions.jl are not compatible with StaticArrays. A GSoC student would adapt the multivariate and matrix-variate distributions as well as the univariate distribution with vector parameters in Distributions.jl to make a spin-off package called StaticDistributions.jl. The student would then benchmark StaticDistributions.jl against Distributions.jl and showcase an example of using StaticDistributions.jl together with CuArrays.jl and/or CUDAnative.jl for GPU-acceleration.</p>

<p><strong>Recommended skills:</strong> An understanding of generated functions in Julia. Some knowledge of random number generators and probablity distributions. An interest in performance optimization and micro-optimization as well as general-purpose GPU programming.</p>

<p><strong>Expected output:</strong> A package StaticDistributions.jl containing implementations of non-allocating multivariate and matrix-variate distributions with vectorized logpdf support, a benchmarking of StaticDistributions.jl against Distributions.jl, and tutorials on how to use StaticDistributions together with CuArrays and the Julia GPU stack.</p>

  </div>
  <br /><br />
  


  <head>
  <meta name="description" content="We thank our contributors, donators, and Fastly for their support in keeping the Julia Language going. Donate here to help pay for Julia's needs."/>
</head>

<footer class="container-fluid footer-copy">
    <div class="container">
      <div class="row">
        <div class="col-md-10 py-2">
          <p>
            We thank <a style="color: #7a95dd" href="https://www.fastly.com">Fastly</a> for their generous infrastructure support. Donations help pay for community resources such as CI, Discourse, workshops, travel, JuliaCon, and other such needs.
          </p>
          <p>
            ©2020 JuliaLang.org contributors. The website content uses the <a style="color: #7a95dd" href="https://github.com/JuliaLang/www.julialang.org/blob/master/LICENSE.md">MIT license</a>.
          </p>
        </div>
        <div class="col-md-2 py-2">
          <a class="github-button" href="https://github.com/sponsors/julialang" data-icon="octicon-heart" data-size="large" aria-label="Sponsor @julialang on GitHub">Sponsor</a>
        </div>
      </div>
    </div>
</footer>


  <script src="../../../v2/js/jquery.min.js"></script>
<script src="../../../v2/js/bootstrap.min.js"></script>
<script src="../../../v2/js/platform.js"></script>
<script src="../../../v2/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>


  <script src="../../../v2/js/jquery.min.js"></script>
<script src="../../../v2/js/bootstrap.min.js"></script>
<script src="../../../v2/js/platform.js"></script>
<script src="../../../v2/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>

</body>

</html>
