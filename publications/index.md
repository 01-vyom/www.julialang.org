---
layout: default
title:  Julia Publications
---

<h1 id="julia-language-and-standard-library">Julia language and standard library</h1>
<p><span class="citation">1. <strong>Julia: A fresh approach to numerical computing</strong>. Jeff Bezanson, Alan Edelman, Stefan Karpinski, Viral B. Shah (2014), <a href="http://arxiv.org/abs/1411.1607" class="uri">http://arxiv.org/abs/1411.1607</a><p><blockquote><em>Abstract:</em> The Julia programming language is gaining enormous popularity. Julia was designed to be easy and fast. Most importantly, Julia shatters deeply established notions widely held in the applied community:1. High-level, dynamic code has to be slow by some sort of law of nature. 2. It is sensible to prototype in one language and then recode in another language. 3. There are parts of a system for the programmer, and other parts best left untouched as they are built by the experts.Julia began with a deep understanding of the needs of the scientific programmer and the needs of the computer in mind. Bridging cultures that have often been distant, Julia combines expertise from computer science and computational science creating a new approach to scientific computing. This note introduces the programmer to the language and the underlying design theory. It invites the reader to rethink the fundamental foundations of numerical computing systems.In particular, there is the fascinating dance between specialization and abstraction. Specialization allows for custom treatment. We can pick just the right algorithm for the right circumstance and this can happen at runtime based on argument types (code selection via multiple dispatch). Abstraction recognizes what remains the same after differences are stripped away and ignored as irrelevant. The recognition of abstraction allows for code reuse (generic programming). A simple idea that yields incredible power. The Julia design facilitates this interplay in many explicit and subtle ways for machine performance and, most importantly, human convenience.</blockquote></p></span> <span class="citation">2. <strong>Experimental multi-threading support for the Julia programming language</strong>. Tobias Knopp (2014), <em>HPTCDL’14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages</em>, 1–5, doi:<a href="http://dx.doi.org/10.1109/HPTCDL.2014.11">10.1109/HPTCDL.2014.11</a><p><blockquote><em>Abstract:</em> Julia is a young programming language that is designed for technical computing. Although Julia is dynamically typed it is very fast and usually yields C speed by utilizing a just-in-time compiler. Still, Julia has a simple syntax that is similar to Matlab, which is widely known as an easy-to-use programming environment. While Julia is very versatile and provides asynchronous programming facilities in the form of tasks (coroutines) as well as distributed multi-process parallelism, one missing feature is shared memory multi-threading. In this paper we present our experiment on introducing multi-threading support in the Julia programming environment. While our implementation has some restrictions that have to be taken into account when using threads, the results are promising yielding almost full speedup for perfectly parallelizable tasks.</blockquote></p></span> <span class="citation">3. <strong>Parallel prefix polymorphism permits parallelization, presentation &amp; proof</strong>. Jiahao Chen, Alan Edelman (2014), <em>HPTCDL’14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages</em>, 47–56, doi:<a href="http://dx.doi.org/10.1109/HPTCDL.2014.9">10.1109/HPTCDL.2014.9</a>, <a href="http://jiahao.github.io/parallel-prefix" class="uri">http://jiahao.github.io/parallel-prefix</a><p><blockquote><em>Abstract:</em> Polymorphism in programming languages enables code reuse. Here, we show that polymorphism has broad applicability far beyond computations for technical computing: parallelism in distributed computing, presentation of visualizations of runtime data flow, and proofs for formal verification of correctness. The ability to reuse a single codebase for all these purposes provides new ways to understand and verify parallel programs.</blockquote></p></span> <span class="citation">4. <strong>Array operators using multiple dispatch: A design methodology for array implementations in dynamic languages</strong>. Jeff Bezanson, Jiahao Chen, Stefan Karpinski, Viral Shah, Alan Edelman (2014), <em>ARRAY’14 Proceedings of ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming</em>, 56–61, doi:<a href="http://dx.doi.org/10.1145/2627373.2627383">10.1145/2627373.2627383</a>, <a href="http://arxiv.org/abs/1407.3845" class="uri">http://arxiv.org/abs/1407.3845</a><p><blockquote><em>Abstract:</em> Arrays are such a rich and fundamental data type that they tend to be built into a language, either in the compiler or in a large low-level library. Defining this functionality at the user level instead provides greater flexibility for application domains not envisioned by the language designer. Only a few languages, such as C++ and Haskell, provide the necessary power to define n-dimensional arrays, but these systems rely on compile-time abstraction, sacrificing some flexibility. In contrast, dynamic languages make it straightforward for the user to define any behavior they might want, but at the possible expense of performance. As part of the Julia language project, we have developed an approach that yields a novel trade-off between flexibility and compile-time analysis. The core abstraction we use is multiple dispatch. We have come to believe that while multiple dispatch has not been especially popular in most kinds of programming, technical computing is its killer application. By expressing key functions such as array indexing using multi-method signatures, a surprising range of behaviors can be obtained, in a way that is both relatively easy to write and amenable to compiler analysis. The compact factoring of concerns provided by these methods makes it easier for user-defined types to behave consistently with types in the standard library.</blockquote></p></span> <span class="citation">5. <strong>Julia: A fast dynamic language for technical computing</strong>. Jeff Bezanson, Stefan Karpinski, Viral B. Shah, Alan Edelman (2012), <a href="http://arxiv.org/abs/1209.5145" class="uri">http://arxiv.org/abs/1209.5145</a><p><blockquote><em>Abstract:</em> Dynamic languages have become popular for scientific computing. They are generally considered highly productive, but lacking in performance. This paper presents Julia, a new dynamic language for technical computing, designed for performance from the beginning by adapting and extending modern programming language techniques. A design based on generic functions and a rich type system simultaneously enables an expressive programming model and successful type inference, leading to good performance for a wide range of programs. This makes it possible for much of the Julia library to be written in Julia itself, while also incorporating best-of-breed C and Fortran libraries.</blockquote></p></span></p>
<h1 id="technical-computing-applications">Technical computing applications</h1>
<p><span class="citation">6. <strong>Julia and the numerical homogenization of PDEs</strong>. Clemens Heitzinger, Gerhard Tulzer (2014), <em>HPTCDL’14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages</em>, 36–40, doi:<a href="http://dx.doi.org/10.1109/HPTCDL.2014.8">10.1109/HPTCDL.2014.8</a><p><blockquote><em>Abstract:</em> We discuss the advantages of using Julia for solving multiscale problems involving partial differential equations (PDEs). Multiscale problems are problems where the coefficients of a PDE oscillate rapidly on a microscopic length scale, but solutions are sought on a much larger, macroscopic domain. Solving multiscale problems requires both a theoretic result, i.e., a homogenization result yielding effective coefficients, as well as numerical solutions of the PDE at the microscopic and the macroscopic length scales.Numerical homogenization of PDEs with stochastic coefficients is especially computationally expensive. Under certain assumptions, effective coefficients can be found, but their calculation involves subtle numerical problems. The computational cost is huge due to the generally large number of stochastic dimensions.Multiscale problems arise in many applications, e.g., in uncertainty quantification, in the rational design of nanoscale sensors, and in the rational design of materials.Our code for the numerical stochastic homogenization of elliptic problems is implemented in Julia. Since multiscale problems pose new numerical problems, it is in any case necessary to develop new numerical codes. Julia is a dynamic language inspired by the Lisp family of languages, it is open-source, and it provides native-code compilation, access to highly optimized linear-algebra routines, support for parallel computing, and a powerful macro system. We describe our experience in using Julia and discuss the advantages of Julia’s features in this problem domain.</blockquote></p></span></p>
<h2 id="numerical-optimization-and-operations-research">Numerical Optimization and Operations Research</h2>
<h3 id="convex.jl"><a href="https://github.com/cvxgrp/Convex.jl">Convex.jl</a></h3>
<p><span class="citation">7. <strong>Convex optimization in julia</strong>. Madeleine Udell, Karanveer Mohan, David Zeng, Jenny Hong, Steven Diamond, Stephen Boyd (2014), <em>HPTCDL’14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages</em>, 18–28, doi:<a href="http://dx.doi.org/10.1109/HPTCDL.2014.5">10.1109/HPTCDL.2014.5</a>, <a href="http://arxiv.org/abs/1410.4821" class="uri">http://arxiv.org/abs/1410.4821</a><p><blockquote><em>Abstract:</em> This paper describes Convex, a convex optimization modeling framework in Julia. Convex translates problems from a user-friendly functional language into an abstract syntax tree describing the problem. This concise representation of the global structure of the problem allows Convex to infer whether the problem complies with the rules of disciplined convex programming (DCP), and to pass the problem to a suitable solver. These operations are carried out in Julia using multiple dispatch, which dramatically reduces the time required to verify DCP compliance and to parse a problem into conic form. Convex then automatically chooses an appropriate backend solver to solve the conic form problem.</blockquote></p></span></p>
<h3 id="stochjump.jl"><a href="https://github.com/joehuchette/StochJuMP.jl">StochJuMP.jl</a></h3>
<p><span class="citation">8. <strong>Parallel algebraic modeling for stochastic optimization</strong>. Joey Huchette, Miles Lubin, Cosmin Petra (2014), <em>HPTCDL’14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages</em>, 29–35, doi:<a href="http://dx.doi.org/10.1109/HPTCDL.2014.6">10.1109/HPTCDL.2014.6</a><p><blockquote><em>Abstract:</em> We present scalable algebraic modeling software, StochJuMP, for stochastic optimization as applied to power grid economic dispatch. It enables the user to express the problem in a high-level algebraic format with minimal boilerplate. StochJuMP allows efficient parallel model instantiation across nodes and efficient data localization. Computational results are presented showing that the model construction is efficient, requiring less than one percent of solve time. StochJuMP is configured with the parallel interior-point solver PIPS-IPM but is sufficiently generic to allow straight forward adaptation to other solvers.</blockquote></p></span></p>
<h3 id="jump.jl"><a href="https://github.com/JuliaOpt/JuMP.jl">JuMP.jl</a></h3>
<p><span class="citation">9. <strong>Computing in operations research using Julia</strong>. Miles Lubin, Iain Dunning (2013), <a href="http://arxiv.org/abs/1312.1431" class="uri">http://arxiv.org/abs/1312.1431</a><p><blockquote><em>Abstract:</em> The state of numerical computing is currently characterized by a divide between highly efficient yet typically cumbersome low-level languages such as C, C++, and Fortran and highly expressive yet typically slow high-level languages such as Python and MATLAB. This paper explores how Julia, a modern programming language for numerical computing which claims to bridge this divide by incorporating recent advances in language and compiler design (such as just-in-time compilation), can be used for implementing software and algorithms fundamental to the field of operations research, with a focus on mathematical optimization. In particular, we demonstrate algebraic modeling for linear and nonlinear optimization and a partial implementation of a practical simplex code. Extensive cross-language benchmarks suggest that Julia is capable of obtaining state-of-the-art performance.</blockquote></p></span></p>
<h2 id="numerical-linear-algebra">Numerical linear algebra</h2>
<h3 id="approxfun.jl"><a href="https://github.com/ApproxFun/ApproxFun.jl">ApproxFun.jl</a></h3>
<p><span class="citation">10. <strong>A practical framework for infinite-dimensional linear algebra</strong>. Sheehan Olver, Alex Townsend (2014), <em>HPTCDL’14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages</em>, 57–62, doi:<a href="http://dx.doi.org/10.1109/HPTCDL.2014.10">10.1109/HPTCDL.2014.10</a>, <a href="http://arxiv.org/abs/1409.5529" class="uri">http://arxiv.org/abs/1409.5529</a><p><blockquote><em>Abstract:</em> We describe a framework for solving a broad class of infinite-dimensional linear equations, consisting of almost banded operators, which can be used to resepresent linear ordinary differential equations with general boundary conditions. The framework contains a data structure on which row operations can be performed, allowing for the solution of linear equations by the adaptive QR approach. The algorithm achieves <span class="math"><em>O</em>(<em>n</em><sup><em>o</em><em>p</em><em>t</em></sup>)</blockquote></p></span> complexity, where <span class="math"><em>n</em><sup><em>o</em></sup><em>p</em><em>t</em></blockquote></p></span> is the number of degrees of freedom required to achieve a desired accuracy, which is determined adaptively. In addition, special tensor product equations, such as partial differential equations on rectangles, can be solved by truncating the operator in the <span class="math"><em>y</em></blockquote></p></span>-direction with <span class="math"><em>n</em><sub><em>y</em></sub></blockquote></p></span> degrees of freedom and using a generalized Schur decomposition to upper triangularize, before applying the adaptive QR approach to the <span class="math"><em>x</em></blockquote></p></span>-direction, requiring <span class="math"><em>O</em>(<em>n</em><sub><em>y</em></sub><sup>2</sup><em>n</em><sub><em>x</em></sub><sup><em>o</em><em>p</em><em>t</em></sup>)</blockquote></p></span> operations. The framework is implemented in the ApproxFun package written in the Julia programming language, which achieves highly competitive computational costs by exploiting unique features of Julia.</blockquote></p></span></p>
<p>Download all the references as a <a href="julia.bib">BibTeX file</a>.</p>
